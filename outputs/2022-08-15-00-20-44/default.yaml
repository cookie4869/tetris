common:
  ft_weight: outputs/2022-08-11-14-54-02/trained_model/tetris_epoch9910_score136700.pt # 以前の学習結果を利用する場合のファイル 
  log_path: tensorboard
model:
  name: DQN # 学習方式 (MLP DQN
  finetune: True # 以前の学習結果を利用する  
state:
  dim: 4
train:
  optimizer: Adam #Adam or SGD
  lr: 1.0e-3 # 学習率
  lr_gamma: 0.1 
  lr_momentum: 0.99
  lr_step_size: 1000
  num_epoch: 3000 # 試行回数
  num_decay_epochs: 3500 # ε収束させる試行回数
  initial_epsilon: 1.0 # ε... 学習が局所解にならないように乱数で動かして最適値する。初期値
  final_epsilon: 1.0e-3 # 上記εを漸減させたときの最終値
  batch_size: 512 # バッチサイズ
  gamma: 0.8
  max_penalty: -1 # 正規化する場合の最小報酬
  target_net: True
  target_copy_intarval: 500
  replay_memory_size: 30000
  double_dqn: True # Dobule DQN 有効化
  reward_clipping: True # 報酬を 1 で正規化、ただし消去報酬のみ
  prioritized_replay: True 
  multi_step_learning: False
  multi_step_num: 3
  reward_list: #These parameter are not normalized. 消去報酬の配分
    - 0  #survival
    - 100 #0 #1block
    - 300 #2block
    - 700 #3block
    - 1300 #4block
    - -1300 #game over
  reward_weight: # 形状報酬の配分
    - 0.00005 #1 #bampiness 
    - 0.0001 #1 #max height
    - 0.0001 #1 #hole_num
tetris:
  board_height: 22
  board_width: 10
  score_list:
    - 0
    - 100
    - 300
    - 700
    - 1300
    - -1300
  max_tetrominoes: 3000
